{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('costsensitiveregression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NotCount</th>\n",
       "      <th>YesCount</th>\n",
       "      <th>ATPM</th>\n",
       "      <th>PFD</th>\n",
       "      <th>PFG</th>\n",
       "      <th>SFD</th>\n",
       "      <th>SFG</th>\n",
       "      <th>WP</th>\n",
       "      <th>WS</th>\n",
       "      <th>AH</th>\n",
       "      <th>AN</th>\n",
       "      <th>Status</th>\n",
       "      <th>FNC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NotCount  YesCount  ATPM    PFD  PFG  SFD  SFG        WP   WS   AH   AN  \\\n",
       "0         2        21   0.0  0.000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0   \n",
       "1        23         0   0.0  0.044  0.0  0.0  0.0  0.306179  0.0  0.0  0.0   \n",
       "2         1        22   0.0  0.000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0   \n",
       "3         5        18   0.0  0.000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0   \n",
       "4         1        22   0.0  0.000  0.0  0.0  0.0  0.000000  0.0  0.0  0.0   \n",
       "\n",
       "   Status  FNC  \n",
       "0       0  0.0  \n",
       "1       1  0.0  \n",
       "2       0  0.0  \n",
       "3       1  0.0  \n",
       "4       0  0.0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NotCount', 'YesCount', 'ATPM', 'PFD', 'PFG', 'SFD', 'SFG', 'WP', 'WS',\n",
       "       'AH', 'AN', 'Status', 'FNC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[['NotCount', 'YesCount', 'ATPM', 'PFD', 'PFG', 'SFD', 'SFG', 'WP', 'WS',\n",
    "       'AH', 'AN']]\n",
    "Y = dataset['Status']\n",
    "FN = dataset['FNC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ten = torch.tensor(X.values, dtype=torch.float32, requires_grad=True)\n",
    "Y_ten = torch.tensor(Y.values, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "TP_ten = torch.tensor([4 for i in range(len(Y_ten))], dtype=torch.float32, requires_grad=True)\n",
    "FP_ten = torch.tensor([4 for i in range(len(TP_ten))], dtype=torch.float32, requires_grad=True)\n",
    "TN_ten = torch.tensor([0 for i in range(len(TP_ten))], dtype=torch.float32, requires_grad=True)\n",
    "FN_ten = torch.tensor(FN.values, dtype=torch.float32, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing test-train 80-20\n",
    "indices = np.arange(len(dataset) , dtype = np.int64)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:int(len(dataset)*0.8)]\n",
    "test_indices = indices[int(len(dataset)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_ten[train_indices]\n",
    "Y_train = Y_ten[train_indices]\n",
    "X_test = X_ten[test_indices]\n",
    "Y_test = Y_ten[test_indices]\n",
    "\n",
    "TP_train = TP_ten[train_indices]\n",
    "TP_test = TP_ten[test_indices]\n",
    "FP_train = FP_ten[train_indices]\n",
    "FP_test = FP_ten[test_indices]\n",
    "TN_train = TN_ten[train_indices]\n",
    "TN_test = TN_ten[test_indices]\n",
    "FN_train = FN_ten[train_indices]\n",
    "FN_test = FN_ten[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostSensitiveRegression(nn.Module):\n",
    "    def __init__(self , input_dim , output_dim):\n",
    "        super(CostSensitiveRegression, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim , output_dim)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostSensitiveLoss(y, y_hat, tp, fp, fn, tn):\n",
    "    loss = torch.mean(y * (y_hat * tp + (1 - y_hat) * fn) + (1 - y) * (y_hat * fp + (1 - y_hat) * tn)).clone().detach()\n",
    "    return  torch.tensor(loss, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanay\\AppData\\Local\\Temp\\ipykernel_15164\\1919002713.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return  torch.tensor(loss, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 539.3566284179688\n",
      "Epoch: 2 Loss: 539.3566284179688\n",
      "Epoch: 3 Loss: 539.3566284179688\n",
      "Epoch: 4 Loss: 539.3566284179688\n",
      "Epoch: 5 Loss: 539.3566284179688\n",
      "Epoch: 6 Loss: 539.3566284179688\n",
      "Epoch: 7 Loss: 539.3566284179688\n",
      "Epoch: 8 Loss: 539.3566284179688\n",
      "Epoch: 9 Loss: 539.3566284179688\n",
      "Epoch: 10 Loss: 539.3566284179688\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 1000\n",
    "num_batches = len(X_train) // batch_size + 1    # to account for the remainder of samples\n",
    "tp = 4\n",
    "fp = 4\n",
    "tn = 0\n",
    "\n",
    "model = CostSensitiveRegression(11 , 1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x_batch = X_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "        y_batch = Y_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "        fn_batch = FN_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "        tn_batch = TN_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "        fp_batch = FP_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "        tp_batch = TP_train[batch * batch_size : (batch + 1) * batch_size]\n",
    "\n",
    "        y_hat_probs = model(x_batch)\n",
    "        y_hat = torch.tensor([1 if i > 0.5 else 0 for i in y_hat_probs])\n",
    "        \n",
    "        loss = CostSensitiveLoss(y_batch, y_hat, tp_batch , fp_batch , fn_batch , tn_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    y_hat_probs = model.forward(X_train)\n",
    "    y_hat = torch.tensor([1 if i > 0.5 else 0 for i in y_hat_probs])\n",
    "    loss = CostSensitiveLoss(Y_train, y_hat, TP_train , TP_train , FN_train , FN_train)\n",
    "    print(\"Epoch: {} Loss: {}\".format(epoch + 1, loss.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxpress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
